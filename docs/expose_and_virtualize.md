# Data Exposure and Virtualization

This example illustrates how the platform can be used to expose and virtualize data. More specifically, two use cases are proposed:

1. Exposing data stored in PostgreSQL via REST and GraphQL
2. Creating a virtual dataset by joining data from heterogeneous sources

The following components are used:

- PostgreSQL with TimescaleDB and PostGIS extensions
- PostgREST
- Hasura
- MinIO
- Dremio

## Deployment

*expose_virtualize.yml* file is used to start the components through Docker/Podman-Compose. You may want to update the services configured in the file if needed (e.g. to change the images).

Open a terminal, `cd` to the *docker-compose* folder and run the following Docker/Podman command:

```shell
podman-compose -p dh -f expose_virtualize.yml up -d
```

## Usage

**NOTE**: the following paragraphs assume you are using the default credentials configured in *expose_virtualize.yml*.

### Postgres

Postgres is preconfigured with the aforementioned extensions and the *docker-compose/resources/db-init-script/postgres-expose-virtualize.sql* script automatically creates the `digitalhub` database. The `test_scenario` schema has a table named `cities`, which contains a list of cities and their geographical coordinates. This script automatically inserts some entries into said table.

### PostgREST

PostgREST exposes the table `cities` as a REST API endpoint. You can query the table using cURL or any other tool. Here are some example queries that can be executed from a terminal:

- `curl "http://localhost:3000/cities"` returns the whole table
- `curl "http://localhost:3000/cities?name=eq.Paris"` returns only the row where column `name` has value `Paris`

Refer to the [PostgREST documentation](https://postgrest.org/en/stable/api.html) for detailed information on the query syntax.

#### Authenticated access
To require authentication, create a web client on AAC, enable *client-secret-basic* as authentication method and *client_credentials* as grant type. Add a custom claim mapping for the *test_scenario_user* role:
```
function claimMapping(claims) {
    claims["role"] = "test_scenario_user";
    return claims;
}
```

Save the client and obtain a client credentials token. Open the *.yml* file: uncomment `PGRST_DB_ANON_ROLE` and edit `PGRST_JWT_SECRET` so that its `n` claim contains the value for the `n` claim presented at *https://<aac_instance>/jwk*. Start a PostgREST container.

You can now call the PostgREST API by adding an `Authorization` header with value `Bearer <your_client_credentials_token>`:
```
curl -H "Authorization: Bearer <your_client_credentials_token>" http://localhost:3000/cities
```

### Hasura

[Hasura](https://github.com/hasura/graphql-engine) provides GraphQL support to Postgres.

Go to *http://localhost:4000* to access the UI. If you enabled authentication, by uncommenting the `HASURA_GRAPHQL_JWT_SECRET` and `HASURA_GRAPHQL_ADMIN_SECRET` variables, you will be asked to provide the value of the second one.

Switch to the *DATA* tab, *View* the `test_scenario` schema, then *Track All* the tables and *Track* (*as Query*) the `find_cities_in` custom function.

Switch back to the *API* tab. You can type queries in the *GraphiQL* section, or easily generate them by using the *Explorer* menu on its left.

Here are some example queries:

- Retrieve `id`, `name`, `location` for cities in the `United Kingdom` ordered by `name`:

```
query MyQuery {
  test_scenario_cities(where: {country: {_eq: "United Kingdom"}}, order_by: {name: asc}) {
    id
    name
    location
  }
}
```

- Find cities contained within a polygon (using the custom function generated by the initialization script):
```
query MyQuery {
  test_scenario_find_cities_in(args: {
    area:"1.85 49.13, 2.96 49.11, 2.92 48.43, 1.82 48.53, 1.85 49.13"
  }) {
    country
    name
  }
}
```

#### Authenticated access
To require authentication, open the *.yml* file and uncomment `HASURA_GRAPHQL_ADMIN_SECRET`. Its value will be asked when accessing the UI, but can also be used when executing queries from tools like curl or Postman, by adding the header `x-hasura-admin-secret: <HASURA_GRAPHQL_ADMIN_SECRET>`.

To use AAC, create a web client, enable *client-secret-basic* as authentication method and *client_credentials* as grant type. Add a custom claim mapping for the *admin* role:
```
function claimMapping(claims) {
    hasura_claims = {};
    hasura_claims["x-hasura-default-role"] = "admin";
    hasura_claims["x-hasura-allowed-roles"] = ["admin"];
    claims["https://hasura.io/jwt/claims"] = hasura_claims;
    return claims;
}
```
Save the client, open the *.yml* file, uncomment `HASURA_GRAPHQL_JWT_SECRET` and, within, specify the AAC instance and update `audience` with the client's ID. Start a Hasura container (you may need to track tables and functions again).

Obtain a token and then try the following curl command, which will query the custom function:
```
curl --location --request POST 'http://localhost:4000/v1/graphql' \
--header 'Authorization: Bearer <your_client_credentials_token>' \
--data-raw '{"query":"query MyQuery {test_scenario_find_cities_in(args: {area:\"1.85 49.13, 2.96 49.11, 2.92 48.43, 1.82 48.53, 1.85 49.13\"}) {country name}}"}'
```

Note that, even if you plan on using AAC, `HASURA_GRAPHQL_ADMIN_SECRET` still needs to be uncommented.

### MinIO

*docker-compose/resources/worldwide-pollution.csv* is a dataset published by the Environmental Protection Agency and provides geolocated information about air quality per country and city. The whole dataset is publicly available on [Opendatasoft](https://public.opendatasoft.com/explore/dataset/worldwide-pollution/information/?disjunctive.country&disjunctive.filename), while *worldwide-pollution.csv* only contains the subset related to France, Italy and the UK.

Go to *http://localhost:9001* and log in with `minioadmin`/`minioadmin`. Create a new bucket named `testbucket`, then upload the *docker-compose/resources/worldwide-pollution.csv* file.

#### Authenticated access
To require authentication for MinIO, create a web client on AAC, enable *client-secret-basic* as authentication method and *authorization_code* as grant type. Add `http://localhost:9001/oauth_callback` as redirect URI. Enable scope `openid` under *API Access*. Add a custom claim mapping:
```
function claimMapping(claims) {
   claims["policy"] = ["consoleAdmin", "readwrite", "diagnostics"]
   return claims;
}
```

Save the client, open the *.yml* file, uncomment `MINIO_IDENTITY_OPENID_CONFIG_URL`, update the AAC instance and the client's ID and secret. Start a MinIO container and go to *http://localhost:9001*. You should now be asked to log in with AAC.

### Dremio

Dremio is used to virtualize data from PostgreSQL and Minio, that is, to create a virtual dataset on top of both data sources by joining their data.

Navigate to *http://localhost:9047* and log in, then proceed to configuring the data sources as illustrated in the following paragraphs.

#### Connect to MinIO

- Click on *Add Source* and select `Amazon S3`
- On *General* configure the following:
    - *Name*: `minio`
    - *AWS Access Key*: `minioadmin`
    - *AWS Access Secret*: `minioadmin`
- On *Advanced Options* check `Enable compatibility mode` and add the following *Connection Properties*:
    - `fs.s3a.path.style.access`: `true`
    - `fs.s3a.endpoint`: `127.0.0.1:9000`

Inside the newly created data source, open the *testbucket* folder and click on *worldwide-pollution.csv*. Change the *Field Delimiter* to `;` and ckeck *Extract Field Names*, then *Save* (this is to be done only once).

#### Connect to Postgres

- Click on *Add Source* and select `PostgreSQL`
- On *General* configure the following:
    - *Name*: `postgres`
    - *Host*: `localhost`
    - *Port*: `5432`
    - *Database Name*: `digitalhub`
    - *Username*: `postgres`
    - *Password*: `postgres`

Inside the newly created data source, you should see `test_scenario` and the table `cities` inside it.

#### Create a virtual dataset

Open the *SQL Runner* and run the following query, which joins the data sources to select London air quality data:

```
SELECT Pollution.city, Cities.country, Cities.location, Pollution.measurement, Pollution.CO, Pollution.O3, Pollution.PM5, Pollution.NO2
FROM postgres_docker.test_scenario.cities Cities
JOIN (
    SELECT City AS city, "date" AS measurement, "Value CO" AS CO, "Value O3" AS O3, "Value PM5" AS PM5, "Value NO2" AS NO2
    FROM minio.testbucket."worldwide-pollution.csv"
    WHERE City = 'London'
) Pollution
ON Cities.name = Pollution.City
```

Click on *Save Script As*, then on *Save View as*, enter a name for the new dataset (e.g., `london_air_quality_data`) and save it on your home space.

Now you can reopen the virtual dataset and query it either from the SQL Runner or the Dremio API.
