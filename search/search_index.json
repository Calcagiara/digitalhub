{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The Digital Hub is an open-source platform for interoperability of data and services, built by integrating several open-source projects to manage, elaborate, expose and serve data through modern standards.</p> <p>This documentation will illustrate the vision and architecture of the Digital Hub, teach you the purpose of its components, how to access and use them, as well as guide you through the process of developing a simple project.</p>"},{"location":"architecture/","title":"Overview and architecture","text":"<p>The DigitalHub platform offers a flexible, fully integrated environment for the development and deployment of full-scale data- and ML- solutions, with unified management and security.</p> <p>The platform at its core is composed by:</p> <ul> <li>The base infrastructure, which offers flexible compute (w/GPU) and storage</li> <li>Dynamic workspaces (w/Jupyter/VSCode/Spark) for interactive computing</li> <li>Workflow and Job services for batch processing</li> <li>Data services to easily define ETL ops and expose data products</li> <li>ML services to train, deploy and monitor ML models</li> </ul> <p>Everything is accessible and usable within a single project.</p>"},{"location":"architecture/#architecture-and-design","title":"Architecture and design","text":"<p>The platform adopts a layered design, where every layer adds functionalities and value on top of the lower layers. From top to bottom:</p> <ul> <li>AI Domain solutions are complex AI applications which integrates different ML and Data products to deliver a full AI product</li> <li>ML Engineering is dedicated to MLOps and the definition, training and usage of ML products</li> <li>Data Engineering is devoted to DataOps, with full support for ETL, storage, schemas, transactions and full data management, with advanced capabilities for profiling, lineage, validation and versioning of datasets</li> <li>Execution infrastructure serves as the base layer</li> </ul> <p></p>"},{"location":"architecture/#design-principles","title":"Design principles","text":"<p>The platform is designed from ground up following modern architectural and operational patterns, to promote consistency, efficiency and quality while preserving the speed and agility required during initial development stages.</p> <p>The principles adopted during the design can be grouped under the following categories.</p>"},{"location":"architecture/#gitops","title":"GitOps","text":"<ul> <li>Everything is code: functions,  operations, configurations</li> <li>Code is versioned!</li> <li>Declarative state for deployments, services</li> </ul>"},{"location":"architecture/#dataops","title":"DataOps","text":"<ul> <li>Datasets are immutable</li> <li>Datasets are versioned</li> <li>Use schemas</li> <li>Idempotent transformations only</li> </ul>"},{"location":"architecture/#mlops","title":"MLOps","text":"<ul> <li>Automate processes</li> <li>Track experiments</li> <li>Ensure reproducibility</li> </ul>"},{"location":"architecture/#resource-optimization","title":"Resource optimization","text":"<ul> <li>Reduce footprint</li> <li>Optimize interactive usage</li> <li>Maximize resource utilization</li> </ul>"},{"location":"architecture/#infrastructure-layer","title":"Infrastructure layer","text":"<p>The infrastructure layer is the foundation of the platform, and offers a dynamic environment to run cpu (and GPU) workloads both as interactive and as scheduled jobs.</p> <p>Based on Kubernetes, the platform supports distributed computing, scheduling, dynamic scaling, monitoring and operation automation for every tool and component of the platform.</p> <p>Infrastructural components are the building blocks for the construction of workspaces used to develop, build and deploy complex solutions.</p> <p></p>"},{"location":"architecture/#infrastructure-compute","title":"Infrastructure: Compute","text":"<p>Managing compute resources is the core task for the infrastructure layer.</p> <p>The DigitalHub adopts Kubernetes as the orchestration system for managing containerized applications: every workload is packaged into a container and executed in one or more pods, taking into account resource requests and constraints.</p> <p></p> <p>Developers can access and leverage hardware resources (e.g. GPU) by declaring the requirement for their workloads, both for interactive and batch computing.</p>"},{"location":"architecture/#infrastructure-data-stores","title":"Infrastructure: Data stores","text":"<p>Data is the foundation of ML systems, and a key aspect of every analytical process.</p> <p>The platform adopts an unified approach, and integrates:</p> <ul> <li> <p>a data lake-house (Minio) as persistence store, for immutable data, both structured and unstructured, with high performance and scalability</p> </li> <li> <p>a database (PostgreSQL) as operational store, for high velocity and mutable data, with ACID transactions, geo-spatial and time-series extensions and horizontal scalability</p> </li> </ul>"},{"location":"architecture/#infrastructure-workspaces","title":"Infrastructure: Workspaces","text":"<p>Workspaces serve as the interactive computing platform for Data- and ML-Ops, executed in the cloud infrastructure. By adopting a workspace manager (Coder), users are able to autonomously create, operate and manage dynamic workspaces based on templates, delivering:</p> <ul> <li>pre-configured working environments</li> <li>customizability</li> <li>resource usage optimization</li> <li>cost-effectiveness</li> </ul> <p></p>"},{"location":"architecture/#infrastructure-api-gateway","title":"Infrastructure: Api gateway","text":"<p>Data- and ML-services are defined and deployed within the perimeter of a given project, and by default are accessible only from the inside.</p> <p></p> <p>An Api Gateway lets users expose them to the outside world, via a simplified interface aimed at:</p> <ul> <li>minimizing the complexity of exposing services on the internet</li> <li>enforcing a minimal level of security</li> <li>automating routing and proxying of HTTP requests</li> </ul> <p>For complex use cases, a fully fledged api gateway should be used (outside the platform)</p>"},{"location":"architecture/#data-engineering","title":"Data engineering","text":"<p>Data engineering is the core layer for the base platform, and covers all the operational aspects of data management, processing and delivery.</p> <p>By integrating modern ETL/ELT pipelines with serverless processing, on top of modern data stores, the DigitalHub delivers a flexible, adaptable and predictable platform for the construction of data products.</p> <p>The diagram represents the data products life-cycle within the DigitalHub.</p> <p></p>"},{"location":"architecture/#dataops-unified-data","title":"DataOps: Unified data","text":"<p>Both structured and unstructured datasets are first-class citizens in the DigitalHub: every data item persisted in the data stores  is registered in the catalogue and is fully addressable, with a unique, global identifier.</p> <p>The platform supports versioning and ACID transactions on datasets, regardlessly of the backing storage.</p>"},{"location":"architecture/#dataops-query-access","title":"DataOps: Query access","text":"<p>The platform adopts Dremio as the (SQL) query engine, offering a unified interface to access structured and semi-structured data stored both in the data lake and the operational database. Dremio is a self-serve platform with web IDE, authentication and authorization, able to deliver data discoverability and dataset lineage. Also with native integration with external tools via ODBC, JDBC, Arrow Flight</p>"},{"location":"architecture/#dataops-interactive-workspaces","title":"DataOps: Interactive workspaces","text":"<p>Every data engineering process starts from data exploration, a task executed interactively by connecting to data sources and performing exploratory analysis.</p> <p>The platform supports dynamic workspaces based on:</p> <ul> <li>JupyterLab, for notebook based analysis</li> <li>SQLPad, for SQL based data access</li> <li>Dremio, for unified data access and analysis</li> <li>VSCode, for a developer-centric, code-based approach</li> </ul>"},{"location":"architecture/#dataops-serverless-computing","title":"DataOps: Serverless computing","text":"<p>Modern serverless platforms enable developers to fully focus on writing business code, by implementing functions which will eventually be executed by the compute layer.</p> <p>There is no need for \u201cexecutable\u201d code: the framework will provide the engine which will transform bare functions into applications.</p> <p></p> <p>The platform adopts Nuclio as the serverless platform, and supports Python, Java and Go as programming languages.</p>"},{"location":"architecture/#dataops-batch-jobs-and-workflows","title":"DataOps: Batch jobs and workflows","text":"<p>By registering functions as jobs we can easily execute them programmatically, based on triggers, or as batch operations dispatched to Kubernetes.</p> <p>We can thus define workflows as pipelines composing functions, by connecting inputs and outputs in a DAG:</p> <ul> <li>Promote re-use of functions</li> <li>Promote composition</li> <li>Promote reproducibility</li> <li>Reduce platform- specific expertise at minimum</li> </ul>"},{"location":"architecture/#dataops-data-services","title":"DataOps: Data services","text":"<p>Datasets can be used to expose services which deliver value to consumers, such as:</p> <ul> <li>REST APIs for integration</li> <li>GraphQL APIs for frontend consumption</li> <li>User consoles for analytics</li> <li>Dashboards for data visualization</li> <li>Reports for evaluation</li> </ul> <p>The platform integrates dedicated, pre-configured tools for the most common use cases, as zero-configuration, one-click deployable services.</p>"},{"location":"architecture/#dataops-data-products","title":"DataOps: Data products","text":"<p>A data product is an autonomous component that contains all data, code, and interfaces to serve the consumer needs.</p> <p>The platform aims at supporting the whole lifecycle of data products, by:</p> <ul> <li>letting developers define ETL processes to acquire and manage data</li> <li>offering stores to track and memorize datasets</li> <li>exposing services to publish data for consumption</li> <li>fully describing data products in metadata files, with versioning and tracking</li> </ul>"},{"location":"architecture/#ml-engineering","title":"ML engineering","text":"<p>Adopting MLOps means embracing the complete lifecycle of ML models, from data preparation and training to experiments tracking and model serving.</p> <p>The platform integrates a suite of tools aimed at covering the full span of operations, enabling ML engineers to not only train and deploy models, but also manage complex pipelines, operations and services.</p> <p>The foundation for the ML layer is the open source MLRun framework, which offers an open MLOps environment for building  ML applications across their lifecycle (depicted in picture)</p> <p></p>"},{"location":"architecture/#mlops-interactive-workspaces","title":"MLOps: Interactive workspaces","text":"<p>The job of a ML engineer is mostly carried out inside an interactive compute environment, where the user performs all the operations related to data preparation, feature extraction, model training and evaluation. The platform adopts JupyterLab, along with MLRun, as interactive compute environment, delivering a pre-configured, fully adaptable workspace for ML tasks.</p>"},{"location":"architecture/#mlops-data-and-features","title":"MLOps: Data and features","text":"<p>Datasets are usually pre-processed by domain experts to define and extract features.</p> <p>By adopting a full-fledged feature store (via MLRun), the platform lets developers:</p> <ul> <li>easily define features during training</li> <li>re-use them during serving</li> <li>ensuring that the very same features are used during the whole ML life-cycle</li> </ul>"},{"location":"architecture/#mlops-model-training-and-automl","title":"MLOps: Model training and AutoML","text":"<p>Model training is a complex task, which requires deep knowledge of the ML model, the underlying library and the execution environment.</p> <p>By integrating the most used ML frameworks, the platform lets developers focus on the actual training, with little to no effort dedicated towards:</p> <ul> <li>tracking experiments and runs</li> <li>collecting and evaluating metrics</li> <li>performing hyperparameter optimization (with distributed GridSearch)</li> </ul> <p>With AutoML, supported models are automatically trained and optimized based on data and features, and eventually deployed as services in a fully automated way.</p>"},{"location":"architecture/#mlops-ml-services","title":"MLOps: ML services","text":"<p>Fine-tuned ML models are used to provide services for external applications, by exposing a dedicated API interface.</p> <p>The platform supports a unified Model Server, which can expose any model built upon a supported framework with a common interface, with optional:</p> <ul> <li>multi-step computation</li> <li>model composition</li> <li>feature access</li> <li>performance tracking</li> <li>drift tracking and analysis</li> </ul>"},{"location":"architecture/#ai-solutions","title":"AI solutions","text":"<p>Building over the base layers, the platform aims at supporting full scale AI solutions, which encompass the full stack of tools and resources required to deliver a specific solution in the domain.</p> <p>Datasets, ML models, workflows, services all contribute to the construction of a fully integrated and replicable AI solution.</p> <p>Note: the AI layer is still in early development phase.</p>"},{"location":"core/","title":"Core","text":"<p>The Digital Hub relies on a number of core software for its essential needs like storage, servicing and deployment of applications.</p>"},{"location":"core/#storage","title":"Storage","text":"<p>The platform uses a unified store composed by two types of storage:</p> <ul> <li>MinIO, a S3-compatible object store as persistence store for (un)structured, immutable data</li> <li>Postgres, a relational database as operational store for mutable data, rich with extensions, including ones for geospatial and time-series data.</li> </ul> <p>By using Dremio, data can be viewed, accessed and queried in a unified way. This component is described in higher detail in its own section.</p>"},{"location":"core/#function-as-a-service","title":"Function-as-a-Service","text":"<p>Nuclio allows deploying and hosting serverless functions, most notably written in Python, but also other languages such as Java or Go, that may be executed as REST APIs. An API gateway can then be created to publish them and optionally require authentication. As Nuclio functions run within containers, Nuclio uses Kubernetes to handle their deployment and availability.</p> <p>Kubeflow is a tool to simplify deploying machine-learning workflows on Kubernetes, by defining DAGs where each step is a function with input/output data.</p>"},{"location":"core/#workspaces","title":"Workspaces","text":"<p>Coder enables organizations to set development tools up in the cloud, so developers can access them conveniently and within a collaborative environment. In the platform, it is used as a hub to deploy and access a number of other components.</p> <p>Among its primary features are templates, which can be used to easily create workspaces and launch tools. The Digital Hub comes with a number of templates already available.</p>"},{"location":"core/#monitoring","title":"Monitoring","text":"<p>TODO - Monitoring features are work-in-progress.</p>"},{"location":"components/dashboard/","title":"Dashboard","text":"<p>The dashboard is a central access point to reach a number of tools that are automatically run when the platform is installed. There are two types of tools: components and monitoring.</p> <p></p> <p>Components</p> <ul> <li>MLRun, a framework for MLOps</li> <li>MinIO, an S3-compatible object datalake</li> <li>Nuclio, a platform for serverless functions</li> <li>Kubeflow, a tool for ML workflows on Kubernetes</li> </ul> <p>Monitoring</p> <p>TODO - Monitoring features are work-in-progress.</p> <ul> <li>K8S Infrastructure: deployed components and services</li> <li>Resources: CPU, traffic, memory usage...</li> <li>Application logs: Log data of components and services</li> </ul> <p>How to access</p> <p>Access your instance of Coder and, under the Workspaces tab, you should see digitalhub-dashboard already listed. Click on it and you will see its logs and, above them, a number of buttons. Click on dashboard.</p> <p>If digital-dashboard isn't already there, you can create it by navigating to Templates and clicking Use template to the right of the dashboard entry.</p>"},{"location":"components/deploying-components/","title":"Deploying components","text":"<p>While core tools of the platform are already up and running after installation, other components have to be individually deployed. This is easily and quickly done by creating workspaces in Coder, by using templates.</p> <p>Open your browser and go to the address of the Coder instance of the Digital Hub platform which should have been provided to you. You will need to sign in and will then be directed to the Workspaces page.</p> <p>Access the Templates tab at the top. Available templates are listed.</p> <p></p> <p>To deploy one of these tools, click its corresponding Use template button. It will ask for a name for the workspace, the owner, and possibly a number of configurations that change depending on the template. More details on each template's configuration will be described in its respective component's section.</p> <p>Once a component has been created, it will appear under the Workspaces tab, but may take a few minutes before it's Running. Then, you can click on it to open its overview, and access the tools it offers by using the buttons above the logs.</p>"},{"location":"components/dremio/","title":"Dremio","text":"<p>Dremio provides unified access to data from heterogeneous sources, combining them and granting the ability to visualize them.</p> <ul> <li>Support for many common sources (relational DBs, NoSQL, Parquet, JSON, Excel, etc.)</li> <li>Run read-only SQL queries and join data independently of source</li> <li>Create virtual datasets from queries</li> </ul> <p>How to access</p> <p>Dremio may be launched from Coder, using its template. It will ask for a Dremio Admin Password. Access Dremio by logging in with username <code>admin</code> and this password.</p> <p>The template automatically configures connections to both MinIO (Object Storage) and Postgres (Database), so you do not need to worry about adding them.</p>"},{"location":"components/dremio/#query-data-sources","title":"Query data sources","text":"<p>Once in, you will notice MinIO and Postgres on the left, under Sources. There may already be some data we can use for a quick test.</p> <p>Click on minio and you may see a file listed to the right or, if you see a folder instead, navigate deeper until you find a file. Click on any file you may have found. If the format was explicitly specified in the name, Dremio will detect it and offer a number of settings for the dataset. If not, try different values of Format until one seems correct. Then, click Save.</p> <p>Dremio will switch to to the SQL Runner and you can run queries against the file you selected.</p> <p></p> <p>Of course, you may also run queries against the Postgres database.</p>"},{"location":"components/dremio/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/grafana/","title":"Grafana","text":"<p>Grafana is a platform for data monitoring and analytics, with support for common relational and time-series databases. It can also connect to Dremio, thanks to a plug-in developed by the Co-Innovation Lab.</p> <ul> <li>Support for Postgres, MySQL, Prometheus, MongoDB, etc.</li> <li>Visualize metrics, graphs, logs</li> <li>Create and customize dashboards</li> </ul> <p>How to access</p> <p>Grafana may be launched from Coder, using its template.</p>"},{"location":"components/grafana/#add-a-data-source","title":"Add a data source","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left.</p> <p></p> <p>Click Administration at the bottom. Click Data sources and then Add new data source.</p> <p>Adding data sources</p> <p>You may also add data sources from the Connections menu.</p> <p>A list of supported data sources will appear.</p>"},{"location":"components/grafana/#postgres","title":"Postgres","text":"<p>Many settings can be changed on the Postgres data source, but let's focus on the ones under PostgreSQL Connection, which you must fill to reach the database.</p> <p>Since Grafana relies on time-series information to provide its monitoring features, ideally you want to create a new database, with tables with this functionality. However, if you just wish to test the tool, you can connect to the database that is created by default.</p> <p>You can recover these values by launching a SQLPad workspace, accessing its Terminal (from the bottom above the logs in Coder) and typing <code>env</code>, which will list all the names and values of the environment variables of the tool.</p> <ul> <li><code>Host</code>: value of <code>SQLPAD_CONNECTIONS__pg__host</code></li> <li><code>Database</code>: value of <code>SQLPAD_CONNECTIONS__pg__name</code> (should be <code>mlrun</code>)</li> <li><code>User</code>: value of <code>SQLPAD_CONNECTIONS__pg__username</code> (should be <code>mlrun</code>)</li> <li><code>Password</code>: value of <code>SQLPAD_CONNECTIONS__pg__password</code></li> </ul> <p>Once you have set these parameters, click Save &amp; test at the bottom, and a green notification confirming database connection was successful should appear. You can click on Explore view to try running some SQL queries on the available tables.</p>"},{"location":"components/grafana/#dremio","title":"Dremio","text":"<p>Dremio workspace</p> <p>You need a Dremio workspace in order to add it as a data source in Grafana. You can create one from Coder.</p> <p>Aside from a <code>Name</code> for the data source, it will ask for the following fields:</p> <ul> <li><code>URL</code>: you can find this in Coder: go to your Dremio workspace and look for an Entrypoint value (next to kubernetes_service), which you can click to copy. It may look similar to: <code>http://dremio-digitalhub-dremio:9047</code>.</li> <li><code>User</code>: <code>admin</code></li> <li><code>Password</code>: whichever Dremio Admin Password you entered when you created the Dremio workspace </li> </ul>"},{"location":"components/grafana/#add-a-dashboard","title":"Add a dashboard","text":"<p>Extend the sidebar by clicking on the three-lines icon in the upper left, click Dashboard and then New &gt; New dashboard.</p> <p>Let's add a simple panel. Click Add visualization and select one of the data sources you added, for example PostgreSQL. Grafana will automatically add a new panel to this new dashboard and open the Edit panel view for it.</p> <p>On the bottom left, you can enter a query for a table in the database. Once you do that and click Run query in the same section, the message Data is missing a time field will likely appear. This is because the table you chose does not have a field for time-series and, by default, new panels are assumed to be for Time series.</p> <p>If you simply click on the Switch to table button that appears, you will see the query's results. More interestingly, if you click Open visualization suggesions, or extend the visualization selection (in the upper right, where it says Time series or Table, depending on whether you clicked Switch to table or not), you will be able to explore a variety visualization options for your query.</p> <p></p>"},{"location":"components/grafana/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/jupyter/","title":"Jupyter","text":"<p>Jupyter is a web-based environment for interactive computing, ideal for testing and prototyping. Some of its main features are:</p> <ul> <li>Several programming languages supported: Python, R, Julia, C++, Scala...</li> <li>Collaborative development environment</li> <li>Mix code snippets with markdown-based documentation, charts and graphs</li> </ul> <p>How to access</p> <p>Jupyter may be launched from Coder, using its template. It will ask for the CPU cores, GB of memory and GB of disk space you wish to dedicate to it. Depending on the computation you need, lower values may turn out to be insufficient, but cores and memory may be changed even while the workspace is running.</p>"},{"location":"components/jupyter/#writing-a-simple-notebook","title":"Writing a simple notebook","text":"<p>When you open Jupyter, the Launcher tab should appear, from which you can create a Python 3 notebook. Once clicked, you can already start writing code. Alternatively, you can also create a notebook by right-clicking within the File Browser to the left and choosing New Notebook.</p> <p>Type some simple code, like <code>print('Hello world!')</code> and press Shift+Enter. Jupyter displays the result of the computation immediately under the code cell and creates a new one.</p> <p></p> <p>You can change the type of a cell by clicking on the drop-down button (Code) at the top of your notebook. By choosing Markdown, you can alternate code and its documentation.</p>"},{"location":"components/jupyter/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/mlrun/","title":"MLRun","text":"<p>MLRun is a MLOps framework for building and managing machine-learning applications and automating the workflow.</p> <ul> <li>Ingest and transform data</li> <li>Develop ML models, train and deploy them</li> <li>Track performance and detect problems</li> </ul> <p>How to access</p> <p>MLRun may be accessed from the dashboard. From its interface, you will be able to monitor projects and workflows.</p>"},{"location":"components/mlrun/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/sqlpad/","title":"SQLPad","text":"<p>SQLPad provides a simple interface for connecting to a database, write and run SQL queries.</p> <ul> <li>Support for Postgres, MySQL, SQLite, etc., plus the ability to support more via ODBC</li> <li>Visualize results quickly with line or bar graphs</li> <li>Save queries and graph settings for further use</li> </ul> <p>How to access</p> <p>SQLPad may be launched from Coder, using its template.</p> <p>The template automatically configures connection to Postgres, so you do not need to worry about setting it up.</p>"},{"location":"components/sqlpad/#running-a-simple-query","title":"Running a simple query","text":"<p>When SQLPad is opened, it will display schemas and their tables to the left, and an empty space to the right to write queries in.</p> <p>Even if freshly deployed, some system tables are already present, so let's run a simple query to test the tool. Copy and paste the following: <pre><code>SELECT * FROM public.pg_stat_statements LIMIT 3;\n</code></pre> This query asks for all (<code>*</code>) fields of <code>3</code> records within the <code>pg_stat_statements</code> table of the <code>public</code> schema. Note that <code>public</code> is the default schema, so you could omit the <code>public.</code> part.</p> <p>Click on Run, and you will notice some results appearing at the bottom.</p> <p></p>"},{"location":"components/sqlpad/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"components/vscode/","title":"Visual Studio Code","text":"<p>Visual Studio Code is a code editor optimized for building and debugging applications.</p> <ul> <li>Built-in Git interaction: compare diffs, commit and push directly from the interface</li> <li>Connect to a remote or containerized environment</li> <li>Many publicly-available extensions for code linting, formatting, connecting to additional services, etc.</li> </ul> <p></p> <p>It can be used to connect remotely to the environment of other tools, for example Jupyter, and work on notebooks from VSCode.</p> <p>Note</p> <p>Although sometimes called Code, which may create confusion, it is a separate software from Coder.</p>"},{"location":"components/vscode/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> </ul>"},{"location":"scenarios/etl/collect/","title":"Collect the data","text":"<p>Create a new folder to store the function's code in: <pre><code>new_folder = 'src'\nif not os.path.exists(new_folder):\n    os.makedirs(new_folder)\n</code></pre></p> <p>Define a function for downloading data as-is and persisting it in the data-lake: <pre><code>%%writefile \"src/download-data.py\"\n\nimport mlrun\nimport pandas as pd\nimport requests\n\n@mlrun.handler(outputs=[\"dataset\"])\ndef downloader(context, url: mlrun.DataItem):\n    # read and rewrite to normalize and export as data\n    df = url.as_df(format='csv',sep=\";\")\n    return df\n</code></pre></p> <p>Register the function in MLRun: <pre><code>project.set_function(\"src/download-data.py\", name=\"download-data\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"downloader\")\n</code></pre></p> <p>Then, execute it (locally) as a test. Note that it may take a few minutes. <pre><code>project.run_function(\"download-data\", inputs={'url':URL}, local=True)\n</code></pre></p> <p>The result will be saved as an artifact in the data store, versioned and addressable with a unique key. By default, this key follows the format <code>&lt;function-name&gt;-&lt;handler&gt;-&lt;output&gt;</code>.</p> <p>Write this key into a variable, so we can read the artifact: <pre><code>DF_KEY = 'store://datasets/demo-etl/download-data-downloader_dataset'\n</code></pre></p> <p>Load the data item and then into a data frame: <pre><code>di = mlrun.get_dataitem(DF_KEY)\ndf = di.as_df()\n</code></pre></p> <p>Run <code>df.head()</code> and, if it prints a few records, you can confirm that the data was properly stored. It's time to process this data.</p>"},{"location":"scenarios/etl/expose/","title":"Expose datasets as API","text":"<p>We define an exposing function to make the data reachable via REST API: <pre><code>%%writefile 'src/api.py'\n\nimport mlrun\nimport pandas as pd\nimport os\n\nDF_URL = os.environ[\"DF_URL\"]\ndf = None\n\n\ndef init_context(context):\n    global df\n    context.logger.info(\"retrieve data from {}\".format(DF_URL))\n    di = mlrun.run.get_dataitem(DF_URL)\n    df = di.as_df()\n\n\ndef handler(context, event):\n    global df\n    if df is None:\n        return context.Response(\n            body=\"\", headers={}, content_type=\"application/json\", status_code=500\n        )\n\n    # mock REST api\n    method = event.method\n    path = event.path\n    fields = event.fields\n\n    id = False\n\n    # pagination\n    page = 0\n    pageSize = 50\n\n    if \"page\" in fields:\n        page = int(fields['page'])\n\n    if \"size\" in fields:\n        pageSize = int(fields['size'])\n\n    if page &lt; 0:\n        page = 0\n\n    if pageSize &lt; 1:\n        pageSize = 1\n\n    if pageSize &gt; 100:\n        pageSize = 100\n\n    start = page * pageSize\n    end = start + pageSize\n    total = len(df)\n\n    if end &gt; total:\n        end = total\n\n    ds = df.iloc[start:end]\n    json = ds.to_json(orient=\"records\")\n\n    res = {\"data\": json, \"page\": page, \"size\": pageSize, \"total\": total}\n\n    return context.Response(\n        body=res, headers={}, content_type=\"application/json\", status_code=200\n    )\n</code></pre></p> <p>Register the function: <pre><code>api_fn = project.set_function(\"src/api.py\", name=\"api\", kind=\"nuclio\", image=\"mlrun/mlrun\", handler='handler')\n</code></pre></p> <p>Configure the function for deployment: <pre><code>DF_KEY = 'store://datasets/demo-etl/process-measures-process_dataset-measures'\napi_fn.set_env(name='DF_URL', value=DF_KEY)\napi_fn.with_requests(mem='64M',cpu=\"250m\")\napi_fn.spec.replicas = 1\nproject.save()\n</code></pre></p> <p>Deploy (may take a few minutes): <pre><code>api_fn.deploy()\n</code></pre></p> <p>Invoke the API and print its results: <pre><code>res = api_fn.invoke(\"/?page=5&amp;size=10\")\nprint(res)\n</code></pre></p> <p>You can also use pandas to load the result in a data frame: <pre><code>rdf = pd.read_json(res['data'], orient='records')\nrdf.head()\n</code></pre></p>"},{"location":"scenarios/etl/expose/#create-an-api-gateway","title":"Create an API gateway","text":"<p>Right now, the API is only accessible from within the environment. To make it accessible from outside, we'll need to create an API gateway.</p> <p>Go to your Coder instance, go to the dashboard and access Nuclio. You will notice a <code>demo-etl</code> project, which we created earlier. When you access it, you will see the <code>demo-etl-api</code> function listed, but click on the API GATEWAYS tab on top instead. Then, click on NEW API GATEWAY.</p> <p>On the left, if you wish, set Authentication to Basic and choose Username and Password.</p> <p>In the middle, set any Name you want. Host must use the same domain as the other components of the Digital Hub. For example, if you access Coder at <code>coder.my-digitalhub-instance.it</code>, the Host field should use a value like <code>demo-etl-api.my-digitalhub-instance.it</code>.</p> <p>On the right, under Primary, you must enter the name of the function, in this case <code>demo-etl-api</code>.</p> <p></p> <p>Save and, after a few moments, you will be able to call the API at the address you entered in Host! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/etl/intro/","title":"ETL scenario introduction","text":"<p>Here we explore a proper, realistic scenario. We collect some data regarding traffic, analyze and transform it, then expose the resulting dataset.</p> <p>Access Jupyter from your Coder instance and create a new notebook. If a Jupyter workspace isn't already available, create one from its template.</p> <p>We'll now start writing code. Copy the snippets of code from here and paste them in your notebook, then execute them with Shift+Enter. After running, Jupyter will create a new code cell.</p>"},{"location":"scenarios/etl/intro/#set-up","title":"Set-up","text":"<p>First, we initialize our environment and create a project.</p> <p>Import required libraries: <pre><code>import mlrun\nimport pandas as pd\nimport requests\nimport os\n</code></pre></p> <p>Load environment variables for MLRun: <pre><code>ENV_FILE = \".mlrun.env\"\nif os.path.exists(ENV_FILE):\n    mlrun.set_env_from_file(ENV_FILE)\n</code></pre></p> <p>Create a MLRun project: <pre><code>PROJECT = \"demo-etl\"\nproject = mlrun.get_or_create_project(PROJECT, \"./\")\n</code></pre></p> <p>Check that the project has been created successfully: <pre><code>print(project)\n</code></pre></p>"},{"location":"scenarios/etl/intro/#peek-at-the-data","title":"Peek at the data","text":"<p>Let's take a look at the data we will work with, which is available in CSV (Comma-Separated Values) format at a remote API.</p> <p>Set the URL to the data and the file name: <pre><code>URL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\nfilename = \"rilevazione-flusso-veicoli-tramite-spire-anno-2023.csv\"\n</code></pre> Download the file and save it locally: <pre><code>with requests.get(URL) as r:\n    with open(filename, \"wb\") as f:\n        f.write(r.content)\n</code></pre></p> <p>Use pandas to read the file into a dataframe: <pre><code>df = pd.read_csv(filename, sep=\";\")\n</code></pre></p> <p>You can now run <code>df.head()</code> to view the first few records of the dataset. They contain information about how many vehicles have passed a sensor (spire), located at specific coordinates, within different time slots. If you wish, use <code>df.dtypes</code> to list the columns and respective types of the data, or <code>df.size</code> to know the data's size in Bytes.</p> <p></p> <p>In the next section, we will collect this data and save it to the object store.</p>"},{"location":"scenarios/etl/pipeline/","title":"Workflow","text":"<p>We define a simple workflow, which will execute all the ETL steps we have seen so far by putting their functions together: <pre><code>%%writefile \"pipeline.py\"\n\nfrom kfp import dsl\nimport mlrun\n\nURL = \"https://opendata.comune.bologna.it/api/explore/v2.1/catalog/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2023/exports/csv?lang=it&amp;timezone=Europe%2FRome&amp;use_labels=true&amp;delimiter=%3B\"\n\n@dsl.pipeline(name=\"Demo ETL pipeline\")\ndef pipeline():\n    project = mlrun.get_current_project()\n\n    downloader = project.run_function(\"download-data\",inputs={'url':URL},outputs=[\"dataset\"])\n\n    process_spire = project.run_function(\"process-spire\",inputs={'di': downloader.outputs[\"dataset\"]})\n\n    process_measures = project.run_function(\"process-measures\",inputs={'di': downloader.outputs[\"dataset\"]})\n</code></pre></p> <p>Register the workflow: <pre><code>project.set_workflow(\"pipeline\",\"./pipeline.py\", handler=\"pipeline\")\n</code></pre></p> <p>And run it, this time remotely: <pre><code>project.run(\"pipeline\")\n</code></pre></p> <p>The last section will describe how to expose this newly obtained dataset as a REST API.</p>"},{"location":"scenarios/etl/process/","title":"Process the data","text":"<p>Raw data, as ingested from the remote API, is usually not suitable for consumption. We'll define a set of functions to process it.</p> <p>Define a function to derive the dataset, group information about spires (<code>id</code>, <code>geolocation</code>, <code>address</code>, <code>name</code>...) and save the result in the store: <pre><code>%%writefile \"src/process-spire.py\"\n\nimport mlrun\nimport pandas as pd\n\nKEYS=['codice spira','longitudine','latitudine','Livello','tipologia','codice','codice arco','codice via','Nome via', 'stato','direzione','angolo','geopoint']\n\n@mlrun.handler(outputs=[\"dataset-spire\"])\ndef process(context, di: mlrun.DataItem):\n    df = di.as_df()\n    sdf= df.groupby(['codice spira']).first().reset_index()[KEYS]\n\n    return sdf\n</code></pre></p> <p>Register the function in MLRun: <pre><code>project.set_function(\"src/process-spire.py\", name=\"process-spire\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"process\")\n</code></pre></p> <p>Run it locally: <pre><code>project.run_function(\"process-spire\", inputs={'di': DF_KEY}, local=True)\n</code></pre></p> <p>The results has been saved as an artifact in the data store. Assign its unique key to a variable, load the data item and convert it to a data frame: <pre><code>SDF_KEY = 'store://datasets/demo-etl/process-spire-process_dataset-spire'\nsdf = mlrun.get_dataitem(SDF_KEY).as_df()\n</code></pre></p> <p>Now you can view the results with <code>sdf.head()</code>.</p> <p>Let's transform the data. We will extract a new data frame, where each record contains the identifier of the spire and how much traffic it detected on a specific date and time slot.</p> <p>A record that looks like this:</p> data codice spira 00:00-01:00 01:00-02:00 ... Nodo a ordinanza stato codimpsem direzione angolo longitudine latitudine geopoint giorno settimana 2023-03-25 0.127 3.88 4 1 90 58 ... 15108 4000/343434 A 125 NO 355.0 11.370234 44.509137 44.5091367043883, 11.3702339463537 Sabato <p>Will become 24 records, each containing the spire's code and recorded traffic within each time slot in a specific date:</p> time codice spira value 2023-03-25 00:00 0.127 3.88 4 1 90 ... ... ... <p>Load the data item into a data frame and remove all columns except for date, spire identifier and recorded values for each time slot: <pre><code>df = mlrun.get_dataitem(DF_KEY).as_df()\nkeys = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\ncolumns=['data','codice spira'] + keys\nrdf = df[columns]\n</code></pre></p> <p>Derive dataset for recorded traffic within each time slot for each spire: <pre><code>ls = []\n\nfor key in keys:\n    k = key.split(\"-\")[0]\n    xdf = rdf[['data','codice spira',key]]\n    xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n    xdf['value'] = xdf[key]\n    vdf = xdf[['time','codice spira','value']]\n    ls.append(vdf)\n\nedf = pd.concat(ls)\n</code></pre></p> <p>You can verify with <code>edf.head()</code> that the derived dataset matches our goal.</p> <p>Let's put this into a function: <pre><code>%%writefile \"src/process-measures.py\"\n\nimport mlrun\nimport pandas as pd\n\nKEYS = ['00:00-01:00', '01:00-02:00', '02:00-03:00', '03:00-04:00', '04:00-05:00', '05:00-06:00', '06:00-07:00', '07:00-08:00', '08:00-09:00', '09:00-10:00', '10:00-11:00', '11:00-12:00', '12:00-13:00', '13:00-14:00', '14:00-15:00', '15:00-16:00', '16:00-17:00', '17:00-18:00', '18:00-19:00', '19:00-20:00', '20:00-21:00', '21:00-22:00', '22:00-23:00', '23:00-24:00']\nCOLUMNS=['data','codice spira']\n\n@mlrun.handler(outputs=[\"dataset-measures\"])\ndef process(context, di: mlrun.DataItem):\n    df = di.as_df()\n    rdf = df[COLUMNS+KEYS]\n    ls = []\n    for key in KEYS:\n        k = key.split(\"-\")[0]\n        xdf = rdf[COLUMNS + [key]]\n        xdf['time'] = xdf.data.apply(lambda x: x+' ' +k)\n        xdf['value'] = xdf[key]\n        ls.append(xdf[['time','codice spira','value']])\n    edf = pd.concat(ls)\n    return edf\n</code></pre></p> <p>Register it: <pre><code>project.set_function(\"src/process-measures.py\", name=\"process-measures\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"process\")\n</code></pre></p> <p>Run it locally: <pre><code>project.run_function(\"process-measures\", inputs={'di': DF_KEY}, local=True)\n</code></pre></p> <p>Inspect the resulting data artifact: <pre><code>MDF_KEY = 'store://datasets/demo-etl/process-measures-process_dataset-measures'\nmdf = mlrun.get_dataitem(MDF_KEY).as_df()\nmdf.head()\n</code></pre></p> <p>Now that we have defined three functions to collect data, process it and extract information, let's put them in a pipeline.</p>"},{"location":"scenarios/ml/deploy/","title":"Deploy and expose the model","text":"<p>Deploying a model is as easy as defining a serverless function, providing the model reference and then deploying.</p> <p>Create a model serving function and provide the model: <pre><code>serving_fn = mlrun.new_function(\"serving\", image=\"python:3.9\", kind=\"serving\", requirements=[\"mlrun[complete]\", \"scikit-learn~=1.2.0\"])\nserving_fn.add_model('cancer-classifier',model_path=trainer_run.outputs[\"model\"], class_name='mlrun.frameworks.sklearn.SklearnModelServer')\n</code></pre></p> <p>Deploy (it may take several minutes): <pre><code>project.deploy_function(serving_fn)\n</code></pre></p> <p>You can now test the endpoint: <pre><code>my_data = {\"inputs\"\n           :[[\n               1.371e+01, 2.083e+01, 9.020e+01, 5.779e+02, 1.189e-01, 1.645e-01,\n               9.366e-02, 5.985e-02, 2.196e-01, 7.451e-02, 5.835e-01, 1.377e+00,\n               3.856e+00, 5.096e+01, 8.805e-03, 3.029e-02, 2.488e-02, 1.448e-02,\n               1.486e-02, 5.412e-03, 1.706e+01, 2.814e+01, 1.106e+02, 8.970e+02,\n               1.654e-01, 3.682e-01, 2.678e-01, 1.556e-01, 3.196e-01, 1.151e-01]\n            ]\n}\nserving_fn.invoke(\"/v2/models/cancer-classifier/infer\", body=my_data)\n</code></pre></p>"},{"location":"scenarios/ml/deploy/#create-an-api-gateway","title":"Create an API gateway","text":"<p>To make the API accessible from outside, we'll need to create an API gateway in Nuclio.</p> <p>Go to your Coder instance, go to the dashboard and access Nuclio. You will notice a <code>demo-ml</code> project, which we created earlier. When you access it, you will see the <code>demo-ml-serving</code> function listed, but click on the API GATEWAYS tab on top instead. Then, click on NEW API GATEWAY.</p> <p>On the left, if you wish, set Authentication to Basic and choose Username and Password.</p> <p>In the middle, set any Name you want. Host must use the same domain as the other components of the Digital Hub. For example, if you access Coder at <code>coder.my-digitalhub-instance.it</code>, the Host field should use a value like <code>demo-ml.my-digitalhub-instance.it</code>.</p> <p>On the right, under Primary, you must enter the name of the function, in this case <code>demo-ml-serving</code>.</p> <p>Save and, after a few moments, you will be able to call the API at the address you entered in Host! If you set Authentication to Basic, don't forget that you have to provide the credentials.</p>"},{"location":"scenarios/ml/intro/","title":"ML scenario introduction","text":"<p>This is a scenario that comes as an official tutorial of MLRun. In fact, you can find the notebook in your Jupyter instance, in <code>/tutorial/01-mlrun-basics.ipynb</code>.</p> <p>We will prepare data, train a model and expose it as a service. Access Jupyter from your Coder instance and create a new notebook.</p> <p>Let's initialize our working environment. Import required libraries: <pre><code>import mlrun\nimport os\n</code></pre></p> <p>Load environment variables for MLRun: <pre><code>ENV_FILE = \".mlrun.env\"\nif os.path.exists(ENV_FILE):\n    mlrun.set_env_from_file(ENV_FILE)\n</code></pre></p> <p>Create a MLRun project: <pre><code>PROJECT = \"demo-ml\"\nproject = mlrun.get_or_create_project(PROJECT, \"./\")\n</code></pre></p> <p>Define the following function, which generates the dataset as required by the model: <pre><code>%%writefile data-prep.py\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\n\nimport mlrun\n\n\n@mlrun.handler(outputs=[\"dataset\", \"label_column\"])\ndef breast_cancer_generator():\n    breast_cancer = load_breast_cancer()\n    breast_cancer_dataset = pd.DataFrame(\n        data=breast_cancer.data, columns=breast_cancer.feature_names\n    )\n    breast_cancer_labels = pd.DataFrame(data=breast_cancer.target, columns=[\"label\"])\n    breast_cancer_dataset = pd.concat(\n        [breast_cancer_dataset, breast_cancer_labels], axis=1\n    )\n\n    return breast_cancer_dataset, \"label\"\n</code></pre></p> <p>Register it: <pre><code>data_gen_fn = project.set_function(\"data-prep.py\", name=\"data-prep\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"breast_cancer_generator\")\nproject.save()\n</code></pre></p> <p>Run it locally: <pre><code>gen_data_run = project.run_function(\"data-prep\", local=True)\n</code></pre></p> <p>You can view the state of the execution with <code>gen_data_run.state()</code> or its output with <code>gen_data_run.outputs</code>. You can see a few records from the output artifact: <pre><code>gen_data_run.artifact(\"dataset\").as_df().head()\n</code></pre></p> <p>We will now proceed to training a model.</p>"},{"location":"scenarios/ml/training/","title":"Training the model","text":"<p>MLRun integrates a set of pre-configured, pre-made functions which support both training and evaluation phases for several frameworks:</p> <ul> <li>SciKit-Learn</li> <li>TensorFlor (and Keras)</li> <li>PyTorch</li> <li>XGBoost</li> <li>LightGBM</li> <li>ONNX</li> </ul> <p>MLRun's auto-trainer can train and evaluate models for supported frameworks, in a fully autonomous and automated way.</p> <p>Import the auto-trainer: <pre><code>trainer = mlrun.import_function('hub://auto_trainer')\n</code></pre></p> <p>Run it on the cluster (it may take a few minutes): <pre><code>trainer_run = project.run_function(trainer,\n    inputs={\"dataset\": gen_data_run.outputs[\"dataset\"]},\n    params = {\n        \"model_class\": \"sklearn.ensemble.RandomForestClassifier\",\n        \"train_test_split_size\": 0.2,\n        \"label_columns\": \"label\",\n        \"model_name\": 'cancer',\n    }, \n    handler='train',\n)\n</code></pre></p> <p>Lastly, we'll deploy and test the model.</p>"},{"location":"tasks/data/","title":"Data and transformations","text":"<p>Here we go briefly over how data and transformations are handled through MLRun.</p>"},{"location":"tasks/data/#data","title":"Data","text":"<p>Two types of store are used: MinIO and Postgres.</p>"},{"location":"tasks/data/#minio","title":"MinIO","text":"<p>MinIO is the persistence store, for immutable data in the form of files. MLRun is already configured to store artifacts in a MinIO bucket.</p> <p>MLRun ensures that every run of a function will receive the latest version of any input dataset and save its output artifacts into the data-lake, MinIO, complete with versions and unique keys. Connection to the data-lake and serialization of data items into storage files are abstracted away from the user.</p> <p>Data items can be read by using MLRun's Python API, for example: <pre><code>mlrun.get_dataitem('store://datasets/demo-etl/download-data-downloader_dataset').as_df().head()\n</code></pre></p>"},{"location":"tasks/data/#postgres","title":"Postgres","text":"<p>Postgres is the operational store, for mutable data, and is rich with extensions, most notably for geo-spatial and time-series data.</p>"},{"location":"tasks/data/#functions","title":"Functions","text":"<p>MLRun functions run as serverless functions on Nuclio: each function is packed into a runnable container and tasks such as allocating resources, monitoring progress and cleaning up are all handled automatically.</p> <p>Functions are written in Python and, once they have been defined, are registered into the project via MLRun's API. For example, a function called <code>downloader</code>, defined in a file <code>src/download-data.py</code>, is registered with: <pre><code>project.set_function(\"src/download-data.py\", name=\"download-data\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"downloader\")\n</code></pre></p> <p>And can be run with: <pre><code>project.run_function(\"download-data\", inputs={'url':URL}, local=True)\n</code></pre></p>"},{"location":"tasks/data/#resources","title":"Resources","text":"<ul> <li>MLRun's official documentation on data artifacts</li> <li>MLRun's official documentation on serverless functions</li> </ul>"},{"location":"tasks/git/","title":"Git best practices","text":"<p>Outside of test cases, projects should be backed by a Git repository, to keep track of their history.</p> <p>As we've seen in the projects section, projects may be created with Git initialized, or loaded by cloning a repository.</p>"},{"location":"tasks/git/#resources","title":"Resources","text":"<ul> <li>MLRun's Git best practices</li> </ul>"},{"location":"tasks/projects/","title":"Projects","text":"<p>A project in MLRun is a container for everything (code, assets, configuration, ...) that concerns an application.</p> <p>Often, they have a structure like this: <pre><code>my-project           # Parent directory of the project (context)\n\u251c\u2500\u2500 data             # Data for local tests\n\u251c\u2500\u2500 docs             # Documentation\n\u251c\u2500\u2500 src              # Source code (functions, libs, workflows)\n\u251c\u2500\u2500 tests            # Unit tests (pytest) for functions\n\u251c\u2500\u2500 project.yaml     # MLRun project spec file\n\u251c\u2500\u2500 README.md        # README file\n\u2514\u2500\u2500 requirements.txt # Default Python requirements file\n</code></pre></p> <p>Projects may be created and managed from the UI, but also by using MLRun's API. Here we look briefly at how to do it from a Jupyter Python notebook. Access your Jupyter instance and create a new notebook.</p> <p>Import the <code>mlrun</code> library: <pre><code>import mlrun\n</code></pre></p>"},{"location":"tasks/projects/#create","title":"Create","text":"<p>A new project is created with new_project: the only required fields are <code>name</code> (<code>my-project</code>) and <code>context</code> (<code>./</code>), which is where project files will be stored; <code>user_project</code> indicates the project's name should be unique per user, while <code>init_git</code> initializes git in the context directory. <pre><code>project = mlrun.new_project(\"my-project\", \"./\", user_project=True, \n                            init_git=True, description=\"my new project\")\n</code></pre></p> <p>You can check in the interface that the project has been created: go to Coder, access the dashboard and, from the Components section, open the MLRun Dashboard.</p>"},{"location":"tasks/projects/#load","title":"Load","text":"<p>You can load a project with load_project, by indicating its <code>context</code>: <pre><code>project = mlrun.load_project(\"./\")\n</code></pre></p> <p>Or clone it from a repository: <pre><code>project = load_project(name=\"my-project\",\n                       url=\"git://github.com/mlrun/project-archive.git\")\n</code></pre></p> <p>With get_or_create_project you can create a project or, if it already exists, load it into the <code>project</code> variable: <pre><code>project = mlrun.get_or_create_project(\"my-project\", \"./\")\n</code></pre></p>"},{"location":"tasks/projects/#delete","title":"Delete","text":"<p>TODO - documentation mentions <code>mlrun.db.delete_project</code>, but this method seems to be missing: <code>AttributeError: module 'mlrun.db' has no attribute 'delete_project'</code></p>"},{"location":"tasks/projects/#resources","title":"Resources","text":"<ul> <li>Official documentation on MLRun projects</li> </ul>"},{"location":"tasks/workspaces/","title":"Workspaces","text":"<p>Workspaces in Coder contain dependencies and configuration required for applications to run.</p> <p>We've already seen how to create a workspace from a template and how to access its applications once it's running, so let's take a look at how to access the workspace in a terminal.</p> <p>The easiest and fastest way is to simply click on the Terminal button above the logs, which will open a terminal in your browser that allows you to browse the workspace's environment.</p> <p></p> <p>If you click on VS Code Desktop, it will open a connection to the workspace in your local instance of VSCode and you can open a terminal by clicking Terminal &gt; New Terminal.</p>"},{"location":"tasks/workspaces/#access-the-workspace-in-a-local-terminal","title":"Access the workspace in a local terminal","text":"<p>You can also connect your local terminal to the workspace via SSH. If you click on SSH, it will show some commands you need to run in your terminal, but first you have to install the <code>coder</code> command and log into the Coder instance.</p> <p>Install <code>coder</code>:</p> Linux / macOSFrom binaries (Windows) <pre><code>curl -fsSL https://coder.com/install.sh | sh\n</code></pre> <p>Download the release for your OS (for example: <code>coder_0.27.2_windows_amd64.zip</code>), unzip it and move the <code>coder</code> executable to a location that's on your <code>PATH</code>. If you need to know how to add a directory to <code>PATH</code>, follow this.</p> <p>Restart the command prompt</p> <p>If it is already running, you will need to restart the Command Prompt for this change to take into effect.</p> <p>Log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre> A tab will open in your browser, ask you to log-in if you haven't already, then display a token that you're supposed to copy and paste in the terminal.</p> <p></p> <p>Now you can run the two commands you saw when you clicked on SSH. Configure SSH hosts (confirm with <code>yes</code> when asked): <pre><code>coder config-ssh\n</code></pre> Note that, if you create new workspaces after running this command, you will need to re-run it to connect to them.</p> <p>The sample command below displays how to connect to the Jupyter workspace and will differ depending on the workspace you want to connect to. Take the actual command from what you see when you click SSH on Coder. <pre><code>ssh coder.jupyter.jupyter\n</code></pre></p> <p>Your terminal should now be connected to the workspace. When you want to terminate the connection, simply type <code>exit</code>. To log coder out, type <code>coder logout</code>.</p>"},{"location":"tasks/workspaces/#port-forwarding","title":"Port-forwarding","text":"<p>Port-forwarding may be done on any port: there are no pre-configured ones and it will work as long as there is a service listening on that port. Ports may be forwarded to make a service public, or through a local session.</p>"},{"location":"tasks/workspaces/#public","title":"Public","text":"<p>This can be done from Coder, directly from the workspace's page. Click on Port forward, enter the port number and click Open URL. Users will have to log in to access the service.</p>"},{"location":"tasks/workspaces/#local","title":"Local","text":"<p>You can start a SSH port-forwarding session from your local terminal. First, log in: <pre><code>coder login https://coder.my-digitalhub-instance.it\n</code></pre></p> <p>The format for the SSH port-forwarding command is: <pre><code>ssh -L [localport]:localhost:[remoteport] coder.[workspace]\n</code></pre></p> <p>For example, it may be: <pre><code>ssh -L 3000:localhost:3000 coder.jupyter.jupyter\n</code></pre></p> <p>You will now be able to access the service in your browser, at <code>localhost:3000</code>.</p>"},{"location":"tasks/workspaces/#resources","title":"Resources","text":"<ul> <li>Official documentation on installation</li> <li>Official documentation on Coder workspaces</li> </ul>"}]}