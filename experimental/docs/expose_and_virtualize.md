# Data Exposure and Virtualization

This example illustrates how the platform can be used to expose and virtualize data. More specifically, two use cases are proposed:

1. Exposing data stored in PostgreSQL via REST and GraphQL
2. Creating a virtual dataset by joining data from heterogeneous sources

The following components are used:

- PostgreSQL with TimescaleDB and PostGIS extensions
- PostgREST
- Hasura
- MinIO
- Dremio

## Deployment

*expose_virtualize.yml* file is used to start the components through Docker/Podman-Compose. You may want to update the services configured in the file if needed (e.g. to change the images).

Open a terminal, `cd` to the *docker-compose* folder and run the following Docker/Podman command:

```shell
podman-compose -p dh -f expose_virtualize.yml up -d
```

## Usage

**NOTE**: the following paragraphs assume you are using the default credentials configured in *expose_virtualize.yml*.

### Postgres

Postgres is preconfigured with the aforementioned extensions and the *docker-compose/resources/db-init-script/postgres-expose-virtualize.sql* script automatically creates the `digitalhub` database. The `test_scenario` schema has a table named `cities`, which contains a list of cities and their geographical coordinates. This script automatically inserts some entries into said table.

### PostgREST

PostgREST exposes the table `cities` as a REST API endpoint. You can query the table using cURL or any other tool. Here are some example queries that can be executed from a terminal:

- `curl "http://localhost:3000/cities"` returns the whole table
- `curl "http://localhost:3000/cities?name=eq.Paris"` returns only the row where column `name` has value `Paris`

Refer to the [PostgREST documentation](https://postgrest.org/en/stable/api.html) for detailed information on the query syntax.

### Hasura

[Hasura](https://github.com/hasura/graphql-engine) provides GraphQL support to Postgres.

Go to *http://localhost:4000* to access the UI. Switch to the *DATA* tab, *View* the `test_scenario` schema, then *Track All* the tables and *Track* (*as Query*) the `find_cities_in` custom function.

Switch back to the *API* tab. You can type queries in the *GraphiQL* section, or easily generate them by using the *Explorer* menu on its left.

Here are some example queries:

- Retrieve `id`, `name`, `location` for cities in the `United Kingdom` ordered by `name`:

```
query MyQuery {
  test_scenario_cities(where: {country: {_eq: "United Kingdom"}}, order_by: {name: asc}) {
    id
    name
    location
  }
}
```

- Find cities contained within a polygon (using the custom function generated by the initialization script):

```
query MyQuery {
  test_scenario_find_cities_in(args: {
    area:"1.85 49.13, 2.96 49.11, 2.92 48.43, 1.82 48.53, 1.85 49.13"
  }) {
    country
    name
  }
}
```

The same queries can be executed with a POST request to the API, for example with cURL:

```
curl --location --request POST 'http://localhost:4000/v1/graphql' \
--data-raw '{"query":"query MyQuery {test_scenario_find_cities_in(args: {area:\"1.85 49.13, 2.96 49.11, 2.92 48.43, 1.82 48.53, 1.85 49.13\"}) {country name}}"}'
```

### MinIO

*docker-compose/resources/worldwide-pollution.csv* is a dataset published by the Environmental Protection Agency and provides geolocated information about air quality per country and city. The whole dataset is publicly available on [Opendatasoft](https://public.opendatasoft.com/explore/dataset/worldwide-pollution/information/?disjunctive.country&disjunctive.filename), while *worldwide-pollution.csv* only contains the subset related to France, Italy and the UK.

Go to *http://localhost:9001* and log in with `minioadmin`/`minioadmin`. Create a new bucket named `testbucket`, then upload the *docker-compose/resources/worldwide-pollution.csv* file.

### Dremio

Dremio is used to virtualize data from PostgreSQL and Minio, that is, to create a virtual dataset on top of both data sources by joining their data.

Navigate to *http://localhost:9047* and log in, then proceed to configuring the data sources as illustrated in the following paragraphs.

#### Connect to MinIO

- Click on *Add Source* and select `Amazon S3`
- On *General* configure the following:
    - *Name*: `Minio`
    - *AWS Access Key*: `minioadmin`
    - *AWS Access Secret*: `minioadmin`
- On *Advanced Options* check `Enable compatibility mode` and add the following *Connection Properties*:
    - `fs.s3a.path.style.access`: `true`
    - `fs.s3a.endpoint`: `127.0.0.1:9000`

Inside the newly created data source, open the *testbucket* folder and click on *worldwide-pollution.csv*. Change the *Field Delimiter* to `;` and check *Extract Field Names*, then *Save* (this is to be done only once).

#### Connect to Postgres

- Click on *Add Source* and select `PostgreSQL`
- On *General* configure the following:
    - *Name*: `Digitalhub`
    - *Host*: `postgres`
    - *Port*: `5432`
    - *Database Name*: `digitalhub`
    - *Username*: `postgres`
    - *Password*: `postgres`

Inside the newly created data source, you should see `test_scenario` and the table `cities` inside it.

#### Create a virtual dataset

Open the *SQL Runner* and run the following query, which joins the data sources to select London air quality data:

```
SELECT Pollution.city, Cities.country, Cities.location, Pollution.measurement, Pollution.CO, Pollution.O3, Pollution.PM5, Pollution.NO2
FROM Digitalhub.test_scenario.cities Cities
JOIN (
    SELECT City AS city, "date" AS measurement, "Value CO" AS CO, "Value O3" AS O3, "Value PM5" AS PM5, "Value NO2" AS NO2
    FROM Minio.testbucket."worldwide-pollution.csv"
    WHERE City = 'London'
) Pollution
ON Cities.name = Pollution.City
```

Click on *Save Script As*, then on *Save View as*, enter a name for the new dataset (e.g., `london_air_quality_data`) and save it on your home space.

Now you can reopen the virtual dataset and query it either from the SQL Runner or the Dremio API.
